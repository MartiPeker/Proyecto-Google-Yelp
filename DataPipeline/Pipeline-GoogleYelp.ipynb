{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2c7c9da",
   "metadata": {},
   "source": [
    "## Importamos las librerias que son necesarias para poder hacer nuestro datapipeline, en este caso vamos a usar las libreria de Kubeflow y Google Cloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baca9e57-c26f-4b03-965b-2106c74f2da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "\n",
    "from typing import NamedTuple, Callable, List, Any, Dict\n",
    "\n",
    "from kfp.dsl import pipeline\n",
    "from kfp.dsl import OutputPath\n",
    "from kfp.dsl import InputPath\n",
    "\n",
    "from kfp.dsl import Output\n",
    "from kfp.dsl import Metrics\n",
    "\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        component,\n",
    "                        Markdown)\n",
    "\n",
    "from kfp import compiler\n",
    "\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform as aip\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "222cc3af",
   "metadata": {},
   "source": [
    "#### Creamos variables para guardar tanto nuestro proyecto_id como nuestra ruta de PIPELINE ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3828c538-1176-4da3-bdb1-74a96689c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "PROJECT_ID = \"project-id\"\n",
    "PIPELINE_ROOT = \"pipe-lineroot\"\n",
    "# Specify BigQuery table details (replace with your own)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d78c2b4",
   "metadata": {},
   "source": [
    "#### Aca inicializamos donde va a estar localizado nuestro proyecto, porque en google cloud necesitamos saber la localizacion de nuestros servicios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0e2b7f4-ea8c-4a5e-86fa-f4b59678542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=\"us-east1\",)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a437122",
   "metadata": {},
   "source": [
    "#### Consultamos las reviews en BigQuery a demás tenemos que usar los diferentes parametros de @component con la necesidad de decirle que imagen base usar, si peude exportar algun tipo de archivo y que paquetes/modulos tiene que instalar para que el componente creado pueda funcionar correctamente \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d86d862b-08ba-4a85-8b06-73234382408a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_3531/2976844999.py:3: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(\n",
      "/var/tmp/ipykernel_3531/2976844999.py:8: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  def query_bigquery(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@component(\n",
    " packages_to_install=[\"google-cloud-bigquery\", \"appengine-python-standard\"],\n",
    " output_component_file=\"bigquery_query.json\",\n",
    " base_image=\"python:3.10\"   \n",
    ")\n",
    "def query_bigquery(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    table_id: str,\n",
    "    year : int,\n",
    "    month : int\n",
    ") -> List:\n",
    "    \"\"\"\n",
    "    Ejecuta una query hacia bigquery para que nos devuelva las reviews que nosotros necesitamos\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The BigQuery project ID.\n",
    "        dataset_id (str): The dataset ID.\n",
    "        table_id (str): The table ID.\n",
    "        year (int): The year to filter reviews by.\n",
    "        month (int): The month to filter reviews by.\n",
    "\n",
    "    Returns:\n",
    "        dict: El resultado de nuestra query sera devuelto en una diccionario\n",
    "    \"\"\"\n",
    "    try :\n",
    "        from google.cloud import bigquery\n",
    "\n",
    "\n",
    "        # Initialize BigQuery client\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "\n",
    "        # Build the query\n",
    "        reviews_query = f\"\"\"\n",
    "          SELECT review_id, text\n",
    "          FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "          WHERE EXTRACT(YEAR FROM date) = {year} AND EXTRACT(MONTH FROM date) = {month} limit 10\n",
    "          \"\"\"\n",
    "\n",
    "        try:\n",
    "          # Execute the query\n",
    "            reviews_query_job = bq_client.query(reviews_query)\n",
    "\n",
    "            result = []\n",
    "            for row in reviews_query_job:\n",
    "                text = row[1]\n",
    "                review_id = row[0]\n",
    "                result.append(\n",
    "                  {'review_id': review_id,\n",
    "                  'text': text,\n",
    "                  })\n",
    "            return result\n",
    "        except Exception as e:\n",
    "              # Handle exceptions (e.g., logging, error handling)\n",
    "              return {\"error\": str(e)}\n",
    "    except Exception as e:\n",
    "          print(\"error\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7a59a-3529-4ca7-8693-953f8eb89122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@component(\n",
    " packages_to_install=[\"google-cloud-bigquery\", \"appengine-python-standard\"],\n",
    " base_image=\"python:3.10\"   \n",
    ")\n",
    "def query_bigquery_google(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    table_id: str,\n",
    "    year : int,\n",
    "    month : int\n",
    ") -> List:\n",
    "    \"\"\"\n",
    "    Ejecuta una query hacia bigquery para que nos devuelva las reviews que nosotros necesitamos\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The BigQuery project ID.\n",
    "        dataset_id (str): The dataset ID.\n",
    "        table_id (str): The table ID.\n",
    "        year (int): The year to filter reviews by.\n",
    "        month (int): The month to filter reviews by.\n",
    "\n",
    "    Returns:\n",
    "        dict: El resultado de nuestra query sera devuelto en una diccionario\n",
    "    \"\"\"\n",
    "    try :\n",
    "        import datetime\n",
    "        def obtener_parte_traducida(texto) -> str : \n",
    "            import re\n",
    "            parte_traducida = re.search(r'\\(Translated by Google\\)(.*?)\\n\\n', texto)\n",
    "            if parte_traducida:\n",
    "                texto = parte_traducida.group(0).strip()\n",
    "                texto_sin_google = texto.replace(\"(Translated by Google)\", \"\").strip()\n",
    "                return texto_sin_google\n",
    "            else:\n",
    "                return texto  # En caso de que no se encuentre, se devuelve el texto original\n",
    "\n",
    "        from google.cloud import bigquery\n",
    "\n",
    "\n",
    "        # Initialize BigQuery client\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "\n",
    "        # Build the query\n",
    "        # Query BigQuery to fetch reviews data\n",
    "        reviews_query = f\"\"\"\n",
    "        SELECT gmap_id, user_id, time, text\n",
    "        FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "        WHERE EXTRACT(YEAR FROM time) = {year} AND EXTRACT(MONTH FROM time) = {month}\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "          # Execute the query\n",
    "            reviews_query_job = bq_client.query(reviews_query)\n",
    "\n",
    "            result = []\n",
    "            for row in reviews_query_job:\n",
    "                gmap_id = row[0]\n",
    "                user_id = row[1]\n",
    "                time = row[2]\n",
    "                time_correx = time.timestamp()\n",
    "                texto = row[3]\n",
    "                if texto :\n",
    "                    texto = obtener_parte_traducida(\n",
    "                    texto)\n",
    "                result.append(\n",
    "                  {\n",
    "                    \"gmap_id\" : gmap_id,\n",
    "                    \"user_id\" : user_id,\n",
    "                    \"time\" : time_correx,\n",
    "                    'text': texto,\n",
    "                  })\n",
    "            return result\n",
    "        except Exception as e:\n",
    "              # Handle exceptions (e.g., logging, error handling)\n",
    "            return [f\"error {e}\"]\n",
    "    except Exception as e:\n",
    "          print(\"error\", str(e))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "302e4d1b",
   "metadata": {},
   "source": [
    "## Consultamos las categorias desde la tabla de BigQuery gracias a una query que creamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ccb2c51-b913-44ae-a870-eb77a8864b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"appengine-python-standard\"],\n",
    "    base_image=\"python:3.10\"   \n",
    ")\n",
    "def query_bigquery_categories(\n",
    "   project_id: str,\n",
    "   dataset_id: str,\n",
    ") -> Dict:\n",
    "\n",
    "    \"\"\"\n",
    "    Ejecuta una Query de Bigquery para obtener las categorias de la base de dato\n",
    "    Args :\n",
    "      project_id (str) : El Proyect Id de BigQuery\n",
    "      dataset_id (str) : El Id del Dataset\n",
    "    \"\"\"\n",
    "    try : \n",
    "        from google.cloud import bigquery\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        categories_table_id = \"Categories\"\n",
    "\n",
    "          # Define your query\n",
    "        categories_query = f\"\"\"\n",
    "              SELECT *\n",
    "              FROM `{project_id}.{dataset_id}.{categories_table_id}`\n",
    "          \"\"\"\n",
    "        try :\n",
    "            # Run the query\n",
    "            categories_query_job = bq_client.query(categories_query)\n",
    "            categories_dictionary = {}\n",
    "\n",
    "            for row in categories_query_job.result():\n",
    "                categories_dictionary[row[0]] = [item['item'] for item in row[1]['list']]\n",
    "            return categories_dictionary\n",
    "        except Exception as e :\n",
    "            return {\"error\": str(e)}\n",
    "    except  Exception as e :\n",
    "        return {\"error\": str(e)}## Instanciamos el modelo del analisis de sentimiento de las reviews, el cual nos devuelve las palabras mas relevantes para nuestro proposito que es encontrar en que tenemos que mejorar, y que fortalezas tenemos\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cf29e65",
   "metadata": {},
   "source": [
    "## Analizamos el Sentimiento de las entidades referido al texto que extraemos de las reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96797dff-f61a-4cd6-bf50-d25f9225d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-language\", \"google-cloud-storage\", \"appengine-python-standard\"], base_image=\"python:3.10\")\n",
    "def all_entity_sentiments(result: List, categories_dictionary: Dict) -> List:\n",
    "    \"\"\"\n",
    "    Ejecuta una función que nos extrae el texto de las reviews, lo insertamos en nuestra funcion\n",
    "    Args:\n",
    "    text (str): El texto que deseamos analizar\n",
    "    diccionario de categoria: El Id del Dataset\n",
    "    Returns:\n",
    "    List: va a contener el diccionario entity_sentiments, que nos va a devolver el sentimiento de diferentes valores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from google.cloud import storage\n",
    "        import json\n",
    "\n",
    "        def analyze_entities_sentiments(text: str, categories_dictionary: dict):\n",
    "            \"\"\"\n",
    "            Ejecuta análisis de entidad y sentimientos para obtener categorías de la base de datos\n",
    "            Args:\n",
    "            text (str): El texto que deseamos analizar\n",
    "            diccionario de categoria: El Id del Dataset\n",
    "            Returns:\n",
    "            Dict: va a contener el diccionario entity_sentiments, que nos va a devolver el sentimiento de diferentes textos y cómo esto influyen a nuestros clientes a la hora de hospedarse\n",
    "            \"\"\"\n",
    "            try:\n",
    "                from google.cloud import language\n",
    "\n",
    "                nlp_client = language.LanguageServiceClient()\n",
    "                document = language.Document(content=text, type_=language.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "                # Analizar entidades en texto de review\n",
    "                entity_analysis = nlp_client.analyze_entities(document=document)\n",
    "\n",
    "                entity_sentiments = {}\n",
    "\n",
    "                # Dividir las entidades y analizar el sentimiento por cada entidad\n",
    "                for entity in entity_analysis.entities:\n",
    "                    entity_text = entity.name\n",
    "                    document = language.Document(content=entity_text, type_=language.Document.Type.PLAIN_TEXT)\n",
    "                    sentiment = nlp_client.analyze_sentiment(document=document)\n",
    "\n",
    "                    # Si la entidad es lo suficientemente relevante\n",
    "                    if 0 < entity.type_ < 8:\n",
    "\n",
    "                        found_match = False\n",
    "\n",
    "                        # Categorizar la entidad\n",
    "                        for key, value in categories_dictionary.items():\n",
    "                            if found_match:\n",
    "                                break\n",
    "                            for item in value:\n",
    "                                if item.casefold() in entity_text.casefold():\n",
    "                                    category = key\n",
    "                                    found_match = True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    category = None\n",
    "\n",
    "                        # Almacenar datos\n",
    "                        entity_sentiments[entity_text] = {\n",
    "                            'category': category,\n",
    "                            'sentiment_score': round(sentiment.document_sentiment.score, 2),\n",
    "                        }\n",
    "                return entity_sentiments\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        all_entity_sentiments = []\n",
    "        for row in result:\n",
    "            text = row[\"text\"]\n",
    "            review_id = row[\"review_id\"]\n",
    "            entity_sentiments_results = analyze_entities_sentiments(text, categories_dictionary)\n",
    "            if entity_sentiments_results:\n",
    "                for entity_text, data in entity_sentiments_results.items():\n",
    "                    all_entity_sentiments.append(\n",
    "                        {'review_id': review_id,\n",
    "                         'entity': entity_text,\n",
    "                         'category': data['category'],\n",
    "                         'sentiment_score': data['sentiment_score'],\n",
    "                         })\n",
    "        json_data = json.dumps(all_entity_sentiments)\n",
    "\n",
    "        bucket_name = \"hoteles-datos\"  # Nombre del bucket (sin \"gs://\")\n",
    "        blob_name = f'PipeLine/json/archivo-yelp.json'\n",
    "\n",
    "        # Crea una instancia del cliente de GCS\n",
    "        storage_client = storage.Client(\"sharp-agent-398819\")\n",
    "\n",
    "        # Obtén un objeto Blob en el bucket\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "\n",
    "        # Sube la cadena JSON al Blob\n",
    "        blob.upload_from_string(json_data, content_type='application/json')\n",
    "\n",
    "        text = [f\"{bucket_name}/{blob_name}\"]\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return print(f\"error\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04149dbf-b041-4102-af9e-aa286f63a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-language\", \"google-cloud-storage\", \"appengine-python-standard\"], base_image=\"python:3.10\")\n",
    "def all_entity_sentiments_google(result: List, categories_dictionary: Dict) -> List:\n",
    "    \"\"\"\n",
    "    Ejecuta una función que nos extrae el texto de las reviews, lo insertamos en nuestra funcion\n",
    "    Args:\n",
    "    text (str): El texto que deseamos analizar\n",
    "    diccionario de categoria: El Id del Dataset\n",
    "    Returns:\n",
    "    List: va a contener el diccionario entity_sentiments, que nos va a devolver el sentimiento de diferentes valores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from google.cloud import storage\n",
    "        import json\n",
    "                \n",
    "        \n",
    "        def analyze_entities_sentiments(text: str, categories_dictionary: dict):\n",
    "            \"\"\"\n",
    "            Ejecuta análisis de entidad y sentimientos para obtener categorías de la base de datos\n",
    "            Args:\n",
    "            text (str): El texto que deseamos analizar\n",
    "            diccionario de categoria: El Id del Dataset\n",
    "            Returns:\n",
    "            Dict: va a contener el diccionario entity_sentiments, que nos va a devolver el sentimiento de diferentes textos y cómo esto influyen a nuestros clientes a la hora de hospedarse\n",
    "            \"\"\"\n",
    "            try:\n",
    "                from google.cloud import language\n",
    "                \n",
    "                \n",
    "                nlp_client = language.LanguageServiceClient()\n",
    "                document = language.Document(content=text, type_=language.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "                # Analizar entidades en texto de review\n",
    "                entity_analysis = nlp_client.analyze_entities(document=document)\n",
    "\n",
    "                entity_sentiments = {}\n",
    "\n",
    "                # Dividir las entidades y analizar el sentimiento por cada entidad\n",
    "                for entity in entity_analysis.entities:\n",
    "                    entity_text = entity.name\n",
    "                    document = language.Document(content=entity_text, type_=language.Document.Type.PLAIN_TEXT)\n",
    "                    sentiment = nlp_client.analyze_sentiment(document=document)\n",
    "\n",
    "                    # Si la entidad es lo suficientemente relevante\n",
    "                    if 0 < entity.type_ < 8:\n",
    "\n",
    "                        found_match = False\n",
    "\n",
    "                        # Categorizar la entidad\n",
    "                        for key, value in categories_dictionary.items():\n",
    "                            if found_match:\n",
    "                                break\n",
    "                            for item in value:\n",
    "                                if item.casefold() in entity_text.casefold():\n",
    "                                    category = key\n",
    "                                    found_match = True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    category = None\n",
    "\n",
    "                        # Almacenar datos\n",
    "                        entity_sentiments[entity_text] = {\n",
    "                            'category': category,\n",
    "                            'sentiment_score': round(sentiment.document_sentiment.score, 2),\n",
    "                        }\n",
    "                return entity_sentiments\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        all_entity_sentiments = []\n",
    "        for row in result:\n",
    "            text = row[\"text\"]\n",
    "            review_id = row[\"gmap_id\"]\n",
    "            user_id = row['user_id']\n",
    "            time = row['time']\n",
    "            entity_sentiments_results = analyze_entities_sentiments(text, categories_dictionary)\n",
    "            if entity_sentiments_results:\n",
    "                for entity_text, data in entity_sentiments_results.items():\n",
    "                    all_entity_sentiments.append(\n",
    "                        {'gmap_id': review_id,\n",
    "                        'user_id': user_id,\n",
    "                        'time': time,\n",
    "                        'entity': entity_text,\n",
    "                        'category': data['category'],\n",
    "                        'sentiment_score': data['sentiment_score'],\n",
    "                         })\n",
    "        json_data = json.dumps(all_entity_sentiments)\n",
    "\n",
    "        bucket_name = \"hoteles-datos\"  # Nombre del bucket (sin \"gs://\")\n",
    "        blob_name = f'PipeLine/json/archivo-google.json'\n",
    "\n",
    "        # Crea una instancia del cliente de GCS\n",
    "        storage_client = storage.Client(\"sharp-agent-398819\")\n",
    "\n",
    "        # Obtén un objeto Blob en el bucket\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "\n",
    "        # Sube la cadena JSON al Blob\n",
    "        blob.upload_from_string(json_data, content_type='application/json')\n",
    "\n",
    "        text = [f\"{bucket_name}/{blob_name}\"]\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return print(f\"error\", str(e))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c471e9c3",
   "metadata": {},
   "source": [
    "## Finalmente cuando pasamos por todos los componentes podemos empezar a insertar los datos que recuperamos de nuestro modelo de NLP a nuestra base de dato (Data WareHouse), para proximamente utilizarlo en el dashboard para analizar nuestro FODA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b1fe10d-7ae6-449f-aca7-6cac98c9cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"google-cloud-storage\" , \"appengine-python-standard\"],\n",
    "    base_image=\"python:3.8\"\n",
    ")\n",
    "def insertar_big_query(url_json: List, project_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Ejecuta una función que nos extrae el texto de las reviews, lo insertamos en nuestra función\n",
    "    Args:\n",
    "        all_entity_sentiments (list): El texto que deseamos analizar\n",
    "        project_id (str): El Id del Dataset\n",
    "    Returns:\n",
    "        Dict: va a contener el diccionario entity_sentiments, que nos va a devolver el sentimiento de diferentes valores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from google.cloud import bigquery\n",
    "        from google.cloud import storage\n",
    "        import json\n",
    "        \n",
    "        # Nombre del archivo JSON en GCS y el nombre del bucket\n",
    "        bucket_name = \"hoteles-datos\"  # Nombre del bucket (sin \"gs://\")\n",
    "        blob_name = f'PipeLine/json/archivo-yelp.json'\n",
    "\n",
    "        # Crea una instancia del cliente de GCS<\n",
    "        storage_client = storage.Client(project=project_id)\n",
    "\n",
    "        # Obtén un objeto Blob en el bucket\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "\n",
    "        # Descarga el contenido del archivo JSON como una cadena\n",
    "        json_data = blob.download_as_text()\n",
    "        \n",
    "        # Convierte la cadena JSON en una lista (o en la estructura que necesites)\n",
    "        all_entity_sentiments = json.loads(json_data)\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        \n",
    "        for entity in all_entity_sentiments:\n",
    "            if entity['category']:\n",
    "                analysis_query = f\"\"\"INSERT INTO `{project_id}.GoogleYelp.Yelp-Review-Analysis` (review_id, entity, category, sentiment_score)\n",
    "                                    VALUES ('{entity['review_id']}', '{entity['entity']}', '{entity['category']}', {entity['sentiment_score']})\n",
    "                                \"\"\"\n",
    "            else:\n",
    "                analysis_query = f\"\"\"INSERT INTO `{project_id}.GoogleYelp.Yelp-Review-Analysis` (review_id, entity, sentiment_score)\n",
    "                                    VALUES ('{entity['review_id']}', '{entity['entity']}', {entity['sentiment_score']})\n",
    "                                \"\"\"\n",
    "            bq_client.query(analysis_query)\n",
    "        \n",
    "        return 'Success!'\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"{e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e2fe356-2ef4-44e4-b469-6bf5b5cd2d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"google-cloud-storage\" , \"appengine-python-standard\"],\n",
    "    base_image=\"python:3.8\"\n",
    ")\n",
    "def insertar_big_query_google(url_json: List, project_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Ejecuta una función que nos extrae el texto de las reviews, lo insertamos en nuestra función\n",
    "    Args:\n",
    "        all_entity_sentiments (list): El texto que deseamos analizar\n",
    "        project_id (str): El Id del Dataset\n",
    "    Returns:\n",
    "        Dict: va a contener el diccionario entity_sentiments, que nos va a devolver el sentimiento de diferentes valores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from google.cloud import bigquery\n",
    "        from google.cloud import storage\n",
    "        import json\n",
    "        \n",
    "        # Nombre del archivo JSON en GCS y el nombre del bucket\n",
    "        bucket_name = \"hoteles-datos\"  # Nombre del bucket (sin \"gs://\")\n",
    "        blob_name = f'PipeLine/json/archivo-google.json'\n",
    "\n",
    "        # Crea una instancia del cliente de GCS<\n",
    "        storage_client = storage.Client(project=project_id)\n",
    "\n",
    "        # Obtén un objeto Blob en el bucket\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "\n",
    "        # Descarga el contenido del archivo JSON como una cadena\n",
    "        json_data = blob.download_as_text()\n",
    "        \n",
    "        # Convierte la cadena JSON en una lista (o en la estructura que necesites)\n",
    "        all_entity_sentiments = json.loads(json_data)\n",
    "        bq_client = bigquery.Client(project=project_id)\n",
    "        \n",
    "        for entity in all_entity_sentiments:\n",
    "            if entity['category']:\n",
    "                analysis_query = f\"\"\"INSERT INTO `{project_id}.GoogleYelp.Google-Review-Analysis` (gmap_id, user_id, time, entity, category, sentiment_score)\n",
    "                                VALUES ('{entity['gmap_id']}', '{entity['user_id']}', TIMESTAMP {entity['time']},'{entity['entity']}', '{entity['category']}', {entity['sentiment_score']})\n",
    "                                \"\"\"\n",
    "            else:\n",
    "                analysis_query = f\"\"\"INSERT INTO `{project_id}.GoogleYelp.Google-Review-Analysis` (gmap_id, user_id, entity, time, sentiment_score)\n",
    "                                VALUES ('{entity['gmap_id']}', '{entity['user_id']}', '{entity['entity']}', TIMESTAMP {entity['time']}, , {entity['sentiment_score']})\n",
    "                                \"\"\"\n",
    "        bq_client.query(analysis_query)\n",
    "        \n",
    "        return 'Success!'\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"{e}\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b856761",
   "metadata": {},
   "source": [
    "### En la siguiente celda podemos ver la manera en como se crea un pipeline con condicionales, a demás este pipeline fue creado con kubeflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9db9e651-7e96-4392-b190-feb86b75b97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<kfp.components.pipeline_task.PipelineTask object at 0x7fbd74b6ab00>\n"
     ]
    }
   ],
   "source": [
    "@dsl.pipeline(name=\"my-pipeline\", pipeline_root=PIPELINE_ROOT)\n",
    "def my_pipeline(\n",
    "    project_id: str = \"project-id\",\n",
    "    dataset_id: str = \"GoogleYelp\",\n",
    "    table_id: str = \"Yelp-Reviews\",\n",
    "    year : int = 2019,\n",
    "    month : int = 9,\n",
    "   \n",
    "):  \n",
    "        def is_yelp_reviews(table_id):\n",
    "            return table_id == \"Yelp-Reviews\"\n",
    "        def is_google_reviews(table_id):\n",
    "            return table_id == \"Google-Reviews\"\n",
    "        \n",
    "        with dsl.Condition(is_yelp_reviews(table_id)): \n",
    "            # Operaciones para consultar BigQuery y obtener datos de la tabla de categorías\n",
    "            bq_data = query_bigquery(\n",
    "                project_id=project_id,\n",
    "                dataset_id=dataset_id,\n",
    "                table_id=table_id,\n",
    "                year = year,\n",
    "                month = month\n",
    "            )\n",
    "\n",
    "            categories_data = query_bigquery_categories(\n",
    "                project_id=project_id,\n",
    "                dataset_id=dataset_id,\n",
    "            )\n",
    "            print()\n",
    "            # Operación para procesar todas las entidades y sentimientos\n",
    "            all_entity_sentiments_result = all_entity_sentiments(\n",
    "                result=bq_data.output,\n",
    "                categories_dictionary=categories_data.output,\n",
    "            )\n",
    "            insertar_bigquery_result = insertar_big_query(\n",
    "                url_json = all_entity_sentiments_result.output,\n",
    "                project_id=project_id,\n",
    "            )\n",
    "        \n",
    "        with dsl.Condition(is_google_reviews(table_id)): \n",
    "            # Operaciones para consultar BigQuery y obtener datos de la tabla de categorías\n",
    "            bq_data = query_bigquery_google(\n",
    "                project_id=project_id,\n",
    "                dataset_id=dataset_id,\n",
    "                table_id=table_id,\n",
    "                   year = year,\n",
    "                month = month\n",
    "            )\n",
    "\n",
    "            categories_data = query_bigquery_categories(\n",
    "                project_id=project_id,\n",
    "                dataset_id=dataset_id,\n",
    "            )\n",
    "            # Operación para procesar todas las entidades y sentimientos\n",
    "            all_entity_sentiments_result = all_entity_sentiments_google(\n",
    "                result=bq_data.output,\n",
    "                categories_dictionary=categories_data.output,\n",
    "            )\n",
    "            insertar_bigquery_result = insertar_big_query_google(\n",
    "                url_json = all_entity_sentiments_result.output,\n",
    "                project_id=project_id,\n",
    "            )\n",
    "        \n",
    "        # Registro de eventos\n",
    "        print(insertar_bigquery_result)     \n",
    "if __name__ == \"__main__\":\n",
    "    import kfp.compiler as compiler\n",
    "    compiler.Compiler().compile(my_pipeline, \"pipeline.json\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "114bafc6",
   "metadata": {},
   "source": [
    "### Y en las siguientes celda creamos las funciones para hacer el llamado al pipeline pero con diferentes parametros para que se cumpla la condicion puesta en nuestra pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d9451a6-979d-4fe6-a114-06724698543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job =  aip.PipelineJob(\n",
    "    display_name=\"hello-world-pipeline\",\n",
    "    template_path = \"pipeline.json\",\n",
    "    job_id =\"yelp5\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching = False,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID,\n",
    "        \"dataset_id\" : \"GoogleYelp\",\n",
    "        \"table_id\" : \"Yelp-Reviews\",\n",
    "    }\n",
    ")\n",
    "\n",
    "job2 =  aip.PipelineJob(\n",
    "    display_name=\"hello-world-pipeline\",\n",
    "    template_path = \"pipeline.json\",\n",
    "    job_id =\"google11\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching = False,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID,\n",
    "        \"dataset_id\" : \"GoogleYelp\",\n",
    "        \"table_id\" : \"Google-Reviews\",\n",
    "    }\n",
    ")\n",
    "job.submit()\n",
    "job2.submit()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
